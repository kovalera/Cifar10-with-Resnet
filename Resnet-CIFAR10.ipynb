{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the data\n",
    "\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. \n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. \n",
    "\n",
    "There are three versions suitable for download, binary, python and matlab versions, for our tutorial we will use the pythong version\n",
    "http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "\n",
    "\n",
    "\n",
    "## Python version\n",
    "The information below is coppied from the CIFAR website http://www.cs.toronto.edu/~kriz/cifar.html  \n",
    "<br />\n",
    "The archive contains the files data_batch_1, data_batch_2, ..., data_batch_5, as well as test_batch. Each of these files is a Python \"pickled\" object produced with cPickle. Here is a python2 routine which will open such a file and return a dictionary:\n",
    "\n",
    "def unpickle(file):  \n",
    "&nbsp;&nbsp;&nbsp;import cPickle  \n",
    "&nbsp;&nbsp;&nbsp;with open(file, 'rb') as fo:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dict = cPickle.load(fo)  \n",
    "&nbsp;&nbsp;&nbsp;return dict  \n",
    "    \n",
    "And a python3 version:  \n",
    "def unpickle(file):  \n",
    "&nbsp;&nbsp;&nbsp;import pickle  \n",
    "&nbsp;&nbsp;&nbsp;with open(file, 'rb') as fo:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dict = pickle.load(fo, encoding='bytes')  \n",
    "&nbsp;&nbsp;&nbsp;return dict\n",
    "    \n",
    "Loaded in this way, each of the batch files contains a dictionary with the following elements:\n",
    "data -- a 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image. The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image.\n",
    "labels -- a list of 10000 numbers in the range 0-9. The number at index i indicates the label of the ith image in the array data.\n",
    "\n",
    "The dataset contains another file, called batches.meta. It too contains a Python dictionary object. It has the following entries:\n",
    "label_names -- a 10-element list which gives meaningful names to the numeric labels in the labels array described above. For example, label_names[0] == \"airplane\", label_names[1] == \"automobile\", etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0 - Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import tarfile\n",
    "import time\n",
    "import numpy as np\n",
    "import cPickle\n",
    "import cv2\n",
    "import h5py\n",
    "import math\n",
    "\n",
    "from tensorflow.python.training import moving_averages\n",
    "\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Download the CIFAR10 data\n",
    "\n",
    "\n",
    "The first step is to get the data, which takes about 160 MB. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function for downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n",
    "homedir = os.path.expanduser('~')\n",
    "dest_directory = homedir +'/Datasets/cifar10'\n",
    "\n",
    "def maybe_download_and_extract():\n",
    "  \"\"\"Download and extract the tarball from Alex's website.\"\"\"\n",
    "  if not os.path.exists(dest_directory):\n",
    "    os.makedirs(dest_directory)\n",
    "  filename = url.split('/')[-1]\n",
    "  filepath = os.path.join(dest_directory, filename)\n",
    "  if not os.path.exists(filepath):\n",
    "    def _progress(count, block_size, total_size):\n",
    "      sys.stdout.write('\\r>> Downloading %s %.1f%%' % (filename,\n",
    "          float(count * block_size) / float(total_size) * 100.0))\n",
    "      sys.stdout.flush()\n",
    "    filepath, _ = urllib.request.urlretrieve(url, filepath, _progress)\n",
    "    print()\n",
    "    statinfo = os.stat(filepath)\n",
    "    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "  extracted_dir_path = os.path.join(dest_directory, 'cifar-10-batches-bin')\n",
    "  if not os.path.exists(extracted_dir_path):\n",
    "    tarfile.open(filepath, 'r:gz').extractall(dest_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Several functions used for processing the cifar data and creating training batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will implement several important functions that are not related to the model itself, but are necessary in order to process the data into the way that we are able to analyse it with our model. Also, in order to be able to work on larger models, we wouldn't like to keep the whole data in the memory, as this will be too heavy for the system and the GPU will not be able to hold such a load, the data will be saved into a hdf5 file, and all the batches will be accessed directly from that file (In the case of CIFAR10 this proceedure is not important, but will be useful when moveing to larger datasets). Some information regarding the hdf5 file system can be gotten from their official website - https://www.hdfgroup.org/HDF5/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpickling of the data from the downloaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = cPickle.load(fo)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the labels into a on-hot vector (e.g. 3 is represented by [0,0,0,1,0,0,0,0,0,0]) which is the representation used by most machine learning discriminators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def onehot(value,out_size):\n",
    "    output = np.zeros((out_size))\n",
    "    output[value] = 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data and processing the images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_cifar10(path):\n",
    "    data_batches = []\n",
    "    labels_batches = []\n",
    "    for i in xrange(1, 6):\n",
    "        dict = unpickle(path + \"/data_batch_\" + str(i))\n",
    "        data = np.array(dict['data'])\n",
    "        labels = dict['labels']\n",
    "        data_batches.extend(data)\n",
    "        labels_batches.extend(labels)\n",
    "\n",
    "    labels_batches = [onehot(v, 10) for v in labels_batches]\n",
    "\n",
    "    data_batches = np.array(data_batches).reshape(-1, 3, 32, 32)\n",
    "    data_batches = data_batches.transpose(0, 2, 3, 1)\n",
    "\n",
    "    data_batches = data_batches.astype(np.float32)\n",
    "    data_batches, mean, std = normalize(data_batches)\n",
    "    trX = np.array(data_batches[:40000])\n",
    "    trY = np.array(labels_batches[:40000])\n",
    "\n",
    "    valX = np.array(data_batches[40000:])\n",
    "    valY = np.array(labels_batches[40000:])\n",
    "\n",
    "    dict = unpickle(path + \"/test_batch\")\n",
    "    teX, _, _ = normalize(np.array(dict['data']).reshape(-1, 3, 32, 32), mean, std)\n",
    "    teX = teX.transpose(0, 2, 3, 1)\n",
    "\n",
    "    teY = dict['labels']\n",
    "    teY = np.array([onehot(v, 10) for v in teY])\n",
    "\n",
    "    return trX, trY, teX, teY, valX, valY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transferring the images and the labels into an hdf5 files and getting them for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_hdf5(trX, trY, teX, teY, valX, valY,path):\n",
    "    f = h5py.File(path+\"/data.hdf5\", \"w\")\n",
    "    htrX = f.create_dataset(\"trX\", trX.shape, dtype='f')\n",
    "    htrX[...]=trX\n",
    "    htrX.attrs['length']=len(trX)\n",
    "    htrY = f.create_dataset(\"trY\", trY.shape, dtype='f')\n",
    "    htrY[...] = trY\n",
    "    hteX = f.create_dataset(\"teX\", teX.shape, dtype='f')\n",
    "    hteX[...] = teX\n",
    "    hteY = f.create_dataset(\"teY\", teY.shape, dtype='f')\n",
    "    hteY[...] = teY\n",
    "    hvalX = f.create_dataset(\"valX\", valX.shape, dtype='f')\n",
    "    hvalX[...] = valX\n",
    "    hvalY = f.create_dataset(\"valY\", valY.shape, dtype='f')\n",
    "    hvalY[...] = valY\n",
    "\n",
    "def get_hdf5():\n",
    "    path = \"./data\"\n",
    "    f = h5py.File(path + \"/data.hdf5\", \"r\")\n",
    "    return f['trX'],f['trY'],f['valX'],f['valY'],f['teX'],f['teY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the train and the test batch, and also the shape of the image to be sent as a variable to the model later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_img_shape():\n",
    "    path = \"./data\"\n",
    "    f = h5py.File(path + \"/data.hdf5\", \"r\")\n",
    "    return list(f['trX'].shape[1:])\n",
    "\n",
    "def get_test_batch(data,labels, batch_size):\n",
    "    test_indices = np.arange(len(data))  # Get A Test Batch\n",
    "    np.random.shuffle(test_indices)\n",
    "    test_indices = list(np.sort(test_indices[0:batch_size]))\n",
    "    with data.astype('float32'):\n",
    "        batch_data = data[test_indices]\n",
    "    with labels.astype('float32'):\n",
    "        batch_labels = labels[test_indices]\n",
    "    return batch_data, batch_labels\n",
    "\n",
    "def get_train_batch(data,labels,indices):\n",
    "    tr_ind = list(np.sort(indices))\n",
    "    with data.astype('float32'):\n",
    "        batch_data = data[tr_ind]\n",
    "    with labels.astype('float32'):\n",
    "        batch_labels = labels[tr_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reduce the overfitting and to add extra images to our system, in the following functions the images are undergoing randomly modifications that make them different from the existing images. First of all we are flipping the images, thus creating double the amount of training data. Secondly we are adding a padding of 5 pixels to the images and randomly cropping the correct size out of them, this way we are able not only to create a much larger dataset of images, but also make our system not sensitive to transitions of the object in the image, if it is higher, lower, to the left or to the right, our classifier should be able to learn that an object is an object, no matter where it is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def horizontal_flip(image, axis):\n",
    "    '''\n",
    "    Flip an image at 50% possibility\n",
    "    :param image: a 3 dimensional numpy array representing an image\n",
    "    :param axis: 0 for vertical flip and 1 for horizontal flip\n",
    "    :return: 3D image after flip\n",
    "    '''\n",
    "    flip_prop = np.random.randint(low=0, high=2)\n",
    "    if flip_prop == 0:\n",
    "        image = cv2.flip(image, axis)\n",
    "\n",
    "    return image\n",
    "\n",
    "def random_crop_and_flip(batch_data, padding_size):\n",
    "    '''\n",
    "    Helper to random crop and random flip a batch of images\n",
    "    :param padding_size: int. how many layers of 0 padding was added to each side\n",
    "    :param batch_data: a 4D batch array\n",
    "    :return: randomly cropped and flipped image\n",
    "    '''\n",
    "    pad_width = ((0, 0), (padding_size, padding_size), (padding_size, padding_size), (0, 0))\n",
    "    batch_data = np.pad(batch_data, pad_width=pad_width, mode='constant', constant_values=0)\n",
    "    cropped_batch = np.zeros(len(batch_data) * 32 * 32 * 3).reshape(\n",
    "        len(batch_data), 32, 32, 3)\n",
    "\n",
    "    for i in range(len(batch_data)):\n",
    "        x_offset = np.random.randint(low=0, high=2 * padding_size, size=1)[0]\n",
    "        y_offset = np.random.randint(low=0, high=2 * padding_size, size=1)[0]\n",
    "        cropped_batch[i, ...] = batch_data[i, ...][x_offset:x_offset+32,\n",
    "                      y_offset:y_offset+32, :]\n",
    "\n",
    "        cropped_batch[i, ...] = horizontal_flip(image=cropped_batch[i, ...], axis=1)\n",
    "\n",
    "    return cropped_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(images, mean=None,std=None):\n",
    "    if mean == None:\n",
    "        mean = np.mean(images)\n",
    "        std = np.std(images)\n",
    "    images = (images - mean) / std\n",
    "    return images, mean,std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Step 2 - Create the Resnet Model\n",
    "\n",
    "In this step we create the model, which will later on be initialized and trained during the training stage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the Resnet model\n",
    "\n",
    "The resnet model is a recent winner of many image processing competitions, designed by the Microsoft Researchers. Its idea is to try and deepen the existing Networks design by adding a residual layer to the designs, with this they are able to redirect the information flow, such that when the network goes deeper, an extra layer of shallow parts is added to the output of the building block. It was proven that with such design the model will be at least as good as the shallow model, as it allows direct information flow from the shallow levels to the deeper ones. The results achieved by this kind of model allow for deeper models to get much higher results than the shallower models on the sme data without degradation\n",
    "![title](img/w04-ResNet.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Design\n",
    "\n",
    "The idea behind the system is that it is modular, it has a residual basic building block which is then stacked on top of previous blocks, the gradients are then backpropagating through the structure as they would in a regular deep convolutional network such as the VGG model. There are two basic blocks, the regular one and the bottleneck, in this tutorial we are going to use the simple basic block, as done by the facebook team's design in torch for the cifar10 data (can be found on github at - https://github.com/facebook/fb.resnet.torch/blob/master/models/resnet.lua)\n",
    "\n",
    "# <center>simple block</center>\n",
    "![title](img/resnetb.jpg)\n",
    "# <center>bottleneck block</center>\n",
    "![title](img/resnetbottleneck.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the various standard functions and layers used by the ResNET model\n",
    "\n",
    "The resnet model relies mainly on three kinds of most common deep learning blocks to build its own basic block. Those blocks are Convolution, Batch Normalization and a Fully connected layer. The following methods are created as wrappers for the use of the TensorFlow models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Some basic functions to get the shape of the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_shape(x):\n",
    "    \"\"\" Get the shape of a Tensor. \"\"\"\n",
    "    return x.get_shape().as_list()\n",
    "def _get_dims(shape):\n",
    "    \"\"\" Get the fan-in and fan-out of a Tensor. \"\"\"\n",
    "    fan_in = np.prod(shape[:-1])\n",
    "    fan_out = shape[-1]\n",
    "    return fan_in, fan_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation and initialization of the weights and biases that will be used by the fully connected and the convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight(name, shape, init='he', range=0.1, stddev=0.01, init_val=None, group_id=0):\n",
    "    \"\"\" Get a weight variable. \"\"\"\n",
    "    if init_val != None:\n",
    "        initializer = tf.constant_initializer(init_val)\n",
    "    elif init == 'uniform':\n",
    "        initializer = tf.random_uniform_initializer(-range, range)\n",
    "    elif init == 'normal':\n",
    "        initializer = tf.random_normal_initializer(stddev=stddev)\n",
    "    elif init == 'he':\n",
    "        fan_in, _ = _get_dims(shape)\n",
    "        std = math.sqrt(2.0 / fan_in)\n",
    "        initializer = tf.random_normal_initializer(stddev=std)\n",
    "    elif init == 'xavier':\n",
    "        fan_in, fan_out = _get_dims(shape)\n",
    "        range = math.sqrt(6.0 / (fan_in + fan_out))\n",
    "        initializer = tf.random_uniform_initializer(-range, range)\n",
    "    else:\n",
    "        initializer = tf.truncated_normal_initializer(stddev = stddev)\n",
    "\n",
    "    var = tf.get_variable(name, shape, initializer=initializer)\n",
    "    tf.add_to_collection('l2_' + str(group_id), tf.nn.l2_loss(var))\n",
    "    return var\n",
    "\n",
    "def bias(name, dim, init_val=0.0):\n",
    "    \"\"\" Get a bias variable. \"\"\"\n",
    "    dims = dim if isinstance(dim, list) else [dim]\n",
    "    return tf.get_variable(name, dims, initializer = tf.constant_initializer(init_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected(x, output_size, name, init_w='he', init_b=0, stddev=0.01, group_id=0):\n",
    "    \"\"\" Apply a fully-connected layer (with bias). \"\"\"\n",
    "    x_shape = _get_shape(x)\n",
    "    input_dim = x_shape[-1]\n",
    "\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        w = weight('weights', [input_dim, output_size], init=init_w, stddev=stddev, group_id=group_id)\n",
    "        b = bias('biases', [output_size], init_b)\n",
    "        z = tf.nn.xw_plus_b(x, w, b)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonlinearity function, can be chosen from three most used ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nonlinear(x, nl=None):\n",
    "    \"\"\" Apply a nonlinearity layer. \"\"\"\n",
    "    if nl == 'relu':\n",
    "        return tf.nn.relu(x)\n",
    "    elif nl == 'tanh':\n",
    "        return tf.tanh(x)\n",
    "    elif nl == 'sigmoid':\n",
    "        return tf.sigmoid(x)\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_norm(x, name, is_train):\n",
    "    \"\"\" Apply a batch normalization layer. \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        inputs_shape = x.get_shape()\n",
    "        axis = list(range(len(inputs_shape) - 1))\n",
    "        param_shape = int(inputs_shape[-1])\n",
    "\n",
    "        moving_mean = tf.get_variable('mean', [param_shape], initializer=tf.constant_initializer(0.0), trainable=False)\n",
    "        moving_var = tf.get_variable('variance', [param_shape], initializer=tf.constant_initializer(1.0), trainable=False)\n",
    "\n",
    "        beta = tf.get_variable('offset', [param_shape], initializer=tf.constant_initializer(0.0))\n",
    "        gamma = tf.get_variable('scale', [param_shape], initializer=tf.constant_initializer(1.0))\n",
    "\n",
    "        control_inputs = []\n",
    "\n",
    "        def mean_var_with_update():\n",
    "            mean, var = tf.nn.moments(x, axis)\n",
    "            update_moving_mean = moving_averages.assign_moving_average(moving_mean, mean, 0.99)\n",
    "            update_moving_var = moving_averages.assign_moving_average(moving_var, var, 0.99)\n",
    "            control_inputs = [update_moving_mean, update_moving_var]\n",
    "            return tf.identity(mean), tf.identity(var)\n",
    "\n",
    "        def mean_var():\n",
    "            mean = moving_mean\n",
    "            var = moving_var\n",
    "            return tf.identity(mean), tf.identity(var)\n",
    "\n",
    "        mean, var = tf.cond(is_train, mean_var_with_update, mean_var)\n",
    "\n",
    "        with tf.control_dependencies(control_inputs):\n",
    "            normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n",
    "\n",
    "    return normed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolution(x, k_h, k_w, c_o, s_h, s_w, name, init_w='he', init_b=0, stddev=0.01, padding='SAME', group_id=0):\n",
    "    \"\"\" Apply a convolutional layer (with bias). \"\"\"\n",
    "    c_i = _get_shape(x)[-1]\n",
    "    convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        w = weight('weights', [k_h, k_w, c_i, c_o], init=init_w, stddev=stddev, group_id=group_id)\n",
    "        z = convolve(x, w)\n",
    "        b = bias('biases', c_o, init_b)\n",
    "        z = tf.nn.bias_add(z, b)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max Pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool(x, k_h, k_w, s_h, s_w, name, padding='SAME'):\n",
    "    \"\"\" Apply a max pooling layer. \"\"\"\n",
    "    return tf.nn.max_pool(x, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions above are the standard deep learning building blocks that are going to be used throughout this tutorial. In the modular implementation in python, those functions are saved in ops.py file, and are imported into the model file. There are many very good resource introducing those methods, and it is very recommended to read up on those before proceeding on with the model implementation. After those are understood, it is trivial to continue on to building the residual network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the ResNET 20 model itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the class ResNET and are initializing all the variables and the shared variables that we will be using throughout the model. In this tutorial we can compare the model results on 4 different optimizers - Momentum, RMSProp, Adagrad and SGD. Their hyperparameters can be added into the initialization, but the optimization of the optimizers is beyond the scope of this tutorial. The cost of the function that is used here is the cross entropy, and since the tensorflow offers a function that already applies the softmax, we do not need to apply it by ourselves to the final output of the process. We are also creating a tensorboard saver in the last few lines of the ResNET, it is used for being able to save checkpoints at different times, and to load them for further processing and analyzing. Also the checkpoint saver is there in order to implement the early stopping algorithm, which saves the best validation model and in the end of the process is loading it, thus stopping our model from overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResNET(object):\n",
    "    def __init__(self, sess,img_shape=[32,32,3],optimizer='RMSProp'):\n",
    "        self.sess = sess\n",
    "        self.is_train = tf.placeholder(tf.bool)\n",
    "        self.lrn_rate = tf.placeholder(tf.float16)\n",
    "        self.batch_size= tf.placeholder(tf.int32)\n",
    "        self.batch_shape = [self.batch_size,128]\n",
    "        self.img_shape = img_shape\n",
    "        self.X = tf.placeholder(\"float\", [None, ]+img_shape)\n",
    "        self.Y = tf.placeholder(\"float\", [None, 10])\n",
    "        self.optimizer = optimizer\n",
    "        self.py_x = self.build_resnet20()\n",
    "        self.out_sum = tf.summary.histogram(\"out\", self.out)\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.py_x, labels=self.Y))\n",
    "        tf.summary.scalar('cross_entropy', self.cost)\n",
    "        if self.optimizer == 'mom':\n",
    "            self.train_op = tf.train.MomentumOptimizer(self.lrn_rate, 0.9).minimize(self.cost)\n",
    "        elif self.optimizer == 'RMSProp':\n",
    "            self.train_op = tf.train.RMSPropOptimizer(self.lrn_rate, 0.9).minimize(self.cost)\n",
    "        elif self.optimizer == \"AdaGrad\":\n",
    "            self.train_op = tf.train.AdagradOptimizer(self.lrn_rate,1e-6).minimize(self.cost)\n",
    "        elif self.optimizer == 'SGD':\n",
    "            self.train_op = tf.train.GradientDescentOptimizer(self.lrn_rate).minimize(self.cost)\n",
    "        else:\n",
    "            self.train_op = self.own_optimizer(self.cost,self.lrn_rate)\n",
    "        self.predict_op = tf.argmax(self.py_x, 1)\n",
    "        with tf.name_scope('accuracy'):\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.predict_op,tf.argmax(self.Y,1)),\"float\"))\n",
    "        tf.summary.scalar('accuracy', self.accuracy)\n",
    "        self.checkpoint_dir = 'checkpoint'\n",
    "        self.saver = tf.train.Saver()\n",
    "        # For early stopping\n",
    "        self.best_acc = 0 # Best Accuracy\n",
    "        self.max_no_imp = 50 # Maximum epochs without improvement on the validation data\n",
    "        self.no_imp_counter = 0 # Counter of no improvement epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now finally for the core part of the tutorial, the Resnet model and it's bulding blocks. For this tutorial as mentioned above only the simple block is implemented, the bottleneck block is used in deeper models which we are not going to implement here. The block looks like the following - \n",
    "![title](img/w04-ResNet2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways of adding the input, as is or through the identity convolution, in this tutorial we have used the identity convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResNET(ResNET):\n",
    "    def simple_block(self, input_feats, name1, name2, n, stride=1):\n",
    "        \"\"\" A basic block of ResNets - used for the smaller versions, for more advanced ones, check the bottleneck version\"\"\"\n",
    "        branchSa_feats = convolution(input_feats, 1, 1, 2 * n, stride, stride, name1 + '_branchSa')\n",
    "        branchSa_feats = batch_norm(branchSa_feats, name2 + '_branchSa',self.is_train)\n",
    "\n",
    "        branchSb_feats = convolution(input_feats, 3, 3, n, stride, stride, name1 + '_branchSb')\n",
    "        branchSb_feats = batch_norm(branchSb_feats, name2 + '_branchSb',self.is_train)\n",
    "        branchSbfeats = nonlinear(branchSb_feats, 'relu')\n",
    "\n",
    "        branchSc_feats = convolution(branchSbfeats, 3, 3, n*2, 1, 1, name1 + '_branchSc')\n",
    "        branchSc_feats = batch_norm(branchSc_feats, name2 + '_branchSc',self.is_train)\n",
    "\n",
    "        output_feats = branchSa_feats + branchSc_feats\n",
    "        output_feats = nonlinear(output_feats, 'relu')\n",
    "        return output_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next those building blocks are stacked on top one another and create a deeper network, the network can go as deep as requested, for simplisity in this tutorial we are using a 20 layer network. After the blocks are stacked on top of each other, the output is flattened by the use of average pooling with a kernel size of [1,8,8,1] and the whole thing is reshaped into a vector, that in turn is fed into a fully connected layer that outputs 10 dimensional vector, which corresponds to our output labels. This output then is flattened to make probabilities of each image belonging to one of the 10 categories (this is done in the cost function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResNET(ResNET):\n",
    "    def build_resnet20(self):\n",
    "        \"\"\" Build the ResNet20 net. \"\"\"\n",
    "        \"\"\" This net can be modified to any other size stride by changing the n, I am recreating\n",
    "        the specific residual network that was used by facebook designed for CIFAR. If want to change\n",
    "        this network to a larger one by using only the basic blocks\"\"\"\n",
    "\n",
    "        imgs = self.X\n",
    "        #imgs = tf.placeholder(tf.float32, [self.batch_size] + self.img_shape)\n",
    "        #is_train = tf.placeholder(tf.bool)\n",
    "        # TODO: Relu and batch normalization separately!!!\n",
    "        conv1_feats = convolution(imgs, 3, 3, 16, 1, 1, 'conv1')\n",
    "        conv1_feats = batch_norm(conv1_feats, 'bn_conv1',self.is_train)\n",
    "        conv1_feats = nonlinear(conv1_feats, 'relu')\n",
    "\n",
    "        res2a_feats = self.simple_block(conv1_feats,'res2a','bn2a', 16)\n",
    "        res3a_feats = self.simple_block(res2a_feats, 'res3a', 'bn3a', 32, 2)\n",
    "        res4a_feats = self.simple_block(res3a_feats, 'res4a', 'bn4a', 64,2)\n",
    "        \n",
    "        # Registration of the output activations for further analysis in tensorboard\n",
    "        self.res2_sum = tf.summary.histogram(\"res2\", res2a_feats)\n",
    "        self.res3_sum = tf.summary.histogram(\"res3\", res3a_feats)\n",
    "        self.res4_sum = tf.summary.histogram(\"res4\", res4a_feats)\n",
    "        \n",
    "        # in torch this is spatial average pool\n",
    "        pool4_feats = tf.nn.avg_pool(res4a_feats, ksize=[1,8,8,1], strides=[1,1,1,1], padding = 'VALID')\n",
    "\n",
    "        res5a_feats_flat = tf.reshape(pool4_feats,self.batch_shape)\n",
    "\n",
    "        res6a_fc_feats = fully_connected(res5a_feats_flat, 10, \"FCa\", init_w='he', init_b=0, stddev=0.01, group_id=0)\n",
    "\n",
    "        self.out = res6a_fc_feats\n",
    "\n",
    "        return self.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the building blocks are in place and the model is defined, we can move on to the training of our data. The training is done iteratively on the training dataset, with a predefined batch size, and tested on a validation and testing data of larger size.  \n",
    "The procedure itself is fairly straightforward, after initializing all the variables defined in the graph of tensorflow, we are iterating over the same procedure for a predifined amount of iterations (epochs) and are modifying our training rate to become smaller and smaller as the time progresses (that hyperparameter is an important one for the achieved results and one that is recommended to modify if trying to get better results on Gradient Descent based methods). Then inside the loop, the data is split into batches, and their indices shuffled, this way we prevent our model from learning a certain order in the training data and overfitting the model. The data batch is processed according to the utilities defined above in the cifar data processing part of the tutorial and is fed into the minimizer function. The results are then saved to the logs for further analysis with tensorboard.  \n",
    "After the training is over, we are loading from the memory the best results we have achieved and are testing it on the new data batch and a testing data (the batch can be defined as 10000 to check it on all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResNET(ResNET):\n",
    "    def train(self,train_batch=128,val_batch=256, test_batch=1024,n_epochs=500):\n",
    "        # Need to initialize all variables\n",
    "        trX, trY, valX, valY, teX, teY = get_hdf5()\n",
    "        self.merged = tf.summary.merge_all()\n",
    "        self.train_writer = tf.summary.FileWriter(\"./logs/train\", self.sess.graph)\n",
    "        self.test_writer = tf.summary.FileWriter(\"./logs/test\", self.sess.graph)\n",
    "\n",
    "        tf.global_variables_initializer().run()\n",
    "        lrn_rate = 0.1\n",
    "        count = 0\n",
    "        total_time = time.time()\n",
    "        i = 0\n",
    "        for i in range(n_epochs):\n",
    "            if i == 20:\n",
    "                lrn_rate=0.01\n",
    "            if i ==80:\n",
    "                lrn_rate=0.001\n",
    "            # Adding randomality to reduce overfitting\n",
    "            train_indices = np.arange(len(trX))\n",
    "            np.random.shuffle(train_indices)\n",
    "            training_batch = zip(range(0, len(trX), train_batch),\n",
    "                                 range(train_batch, len(trX) + 1, train_batch))\n",
    "            for start, end in training_batch:\n",
    "                tr_ind = list(np.sort(train_indices[start:end]))\n",
    "                with trX.astype('float32'):\n",
    "                    batch_data = trX[tr_ind]\n",
    "                with trY.astype('float32'):\n",
    "                    batch_labels=trY[tr_ind]\n",
    "                #batch_data, batch_labels = get_train_batch(trX,trY,train_indices[start:end])\n",
    "                # Adding random modification to reduce overfitting\n",
    "                batch_data = random_crop_and_flip(batch_data,5)\n",
    "                summary,_,tr_acc, tr_cost = self.sess.run([self.merged, self.train_op, self.accuracy, self.cost], feed_dict={self.X: batch_data, self.Y: batch_labels,\n",
    "                                                   self.is_train:True, self.lrn_rate:lrn_rate,self.batch_size:train_batch})\n",
    "\n",
    "                self.train_writer.add_summary(summary, count)\n",
    "                count+=1\n",
    "            batch_data, batch_labels = get_test_batch(valX,valY, val_batch)\n",
    "            summary, val_acc = self.sess.run([self.merged, self.accuracy], feed_dict={self.X: batch_data,\n",
    "                                                                                 self.Y: batch_labels,\n",
    "                                                                                 self.is_train: True,\n",
    "                                                                                 self.batch_size: val_batch})\n",
    "            batch_data = None\n",
    "            if val_acc > self.best_acc:\n",
    "                self.save(self.checkpoint_dir, count)\n",
    "                self.no_imp_counter = 0\n",
    "                self.best_acc = val_acc\n",
    "            else:\n",
    "                self.no_imp_counter += 1\n",
    "                if self.no_imp_counter > self.max_no_imp:\n",
    "                    break\n",
    "\n",
    "            self.test_writer.add_summary(summary,count)\n",
    "            #self.train_writer.add_summary(summary, i)\n",
    "            print('train ', i, tr_acc)\n",
    "            #self.test_writer.add_summary(summary, i)\n",
    "            print('validation ',i, val_acc)\n",
    "            print ('cost - ',tr_cost)\n",
    "            if (i%10==0):\n",
    "                batch_data,batch_labels = get_test_batch(teX,teY,test_batch)\n",
    "                summary, test_acc = self.sess.run([self.merged, self.accuracy], feed_dict={self.X: batch_data,\n",
    "                                                                       self.Y: batch_labels,\n",
    "                                                                       self.is_train: True,\n",
    "                                                                       self.batch_size: test_batch})\n",
    "                print('test ', i, test_acc)\n",
    "        total_time -= time.time()\n",
    "        # Print out the best run (early stopping implementation)\n",
    "        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n",
    "        if could_load:\n",
    "            batch_data, batch_labels = get_test_batch(teX,teY, test_batch)\n",
    "            train_data, train_labels = get_test_batch(trX, trY, test_batch)\n",
    "            val_data, val_labels = get_test_batch(valX, valY, test_batch)\n",
    "\n",
    "            val_acc = self.sess.run([self.accuracy], feed_dict={self.X: val_data,\n",
    "                                                                self.Y: val_labels,\n",
    "                                                                self.is_train: True,\n",
    "                                                                self.batch_size: test_batch})\n",
    "            tr_acc = self.sess.run([self.accuracy], feed_dict={self.X: train_data,\n",
    "                                                                self.Y: train_labels,\n",
    "                                                                self.is_train: True,\n",
    "                                                                self.batch_size: test_batch})\n",
    "            te_acc = self.sess.run([self.accuracy], feed_dict={self.X: batch_data,\n",
    "                                                               self.Y: batch_labels,\n",
    "                                                               self.is_train: True,\n",
    "                                                               self.batch_size: test_batch})\n",
    "            print ('average time per iteration is: ', total_time / (i + 1))\n",
    "            print ('best validation accuracy - ',val_acc)\n",
    "            print ('with train accuracy of - ', tr_acc)\n",
    "            print ('with test accuracy of - ', te_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three more small functions that are taking care of loading and saving the checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResNET(ResNET):\n",
    "    @property\n",
    "    def model_dir(self):\n",
    "        return \"\".format()\n",
    "\n",
    "    def save(self, checkpoint_dir, step):\n",
    "        model_name = \"DCGAN.model\"\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        self.saver.save(self.sess,\n",
    "                        os.path.join(checkpoint_dir, model_name),\n",
    "                        global_step=step)\n",
    "\n",
    "    def load(self, checkpoint_dir):\n",
    "        import re\n",
    "        print(\" [*] Reading checkpoints...\")\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "            counter = int(next(re.finditer(\"(\\d+)(?!.*\\d)\", ckpt_name)).group(0))\n",
    "            print(\" [*] Success to read {}\".format(ckpt_name))\n",
    "            return True, counter\n",
    "        else:\n",
    "            print(\" [*] Failed to find a checkpoint\")\n",
    "            return False, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, tutorial almost over, the model is implemented, training procedure defined and all the parts are in the right place. We can proceed on to the main function which will set the wheels in motion on our journey into the analysis of the Cifar10 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "#flags.DEFINE_integer(\"n_epochs\", 500, \"Epoch to train [500]\")\n",
    "#flags.DEFINE_integer(\"train_size\", 128, \"The size of batch images [128]\")\n",
    "#flags.DEFINE_integer(\"val_size\", 256, \"The size of batch images [128]\")\n",
    "#flags.DEFINE_integer(\"test_size\", 512, \"The size of batch images [128]\")\n",
    "#flags.DEFINE_string(\"optimizer\",\"mom\",\"The name of the optimizer to be used [AdaDelta,mom,RMSProp,SGD]\")\n",
    "#FLAGS = flags.FLAGS\n",
    "n_epochs = 500\n",
    "train_size = 128\n",
    "val_size = 256\n",
    "test_size = 512\n",
    "optimizer = \"mom\"\n",
    "def main(_):\n",
    "    homedir = os.path.expanduser('~')\n",
    "    path = homedir + '/Datasets/cifar10/cifar-10-batches-py'\n",
    "    hdf5_path = \"./data\"\n",
    "\n",
    "    if not os.path.exists(hdf5_path+\"/data.hdf5\"):\n",
    "        print (\"hdf5 doesn't exist, creating it in data folder\")\n",
    "        trX, trY, teX, teY, valX, valY = read_cifar10(path)\n",
    "        write_hdf5(trX, trY, teX, teY, valX, valY, hdf5_path)\n",
    "\n",
    "    run_config = tf.ConfigProto()\n",
    "    run_config.gpu_options.allow_growth = True\n",
    "    with tf.Session(config=run_config) as sess:\n",
    "        ResNet_model = ResNET(sess, img_shape=get_img_shape(),optimizer=FLAGS.optimizer)\n",
    "        ResNet_model.train(n_epochs=n_epochs,train_batch=train_size,val_batch=val_size, test_batch=test_size)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is it for this tutorial, the test results we were able to achieve on the Cifar10 are roughly the same as were illustrated by the original paper (after about 100 epochs a training accuracy of 93% and a testing accuracy of 92%). This structures can be further applied to other datasets, and by using the basic building blocks of the residual model create deeper and stronger models. There are many parts of the design that can be modified and perhaps even improved, but that is left as an exercise to the student."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
